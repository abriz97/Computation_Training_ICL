---
title: "Computational Training: running fastR"

author: "Andrea Brizzi"
date: 2024-12-12
# bibliography: references.bib
execute:
    echo: true
    eval: false
format:
    revealjs:
        title-slide-attributes:
            data-background-image: "../figures/IMPERIAL_I_Maze_RGB.svg"
            data-background-size: 15%
            data-background-position: 50% 98%
        preload-iframes: true
        embed-resources: true
        default-fig-align: "center"
        slide-number: true
        theme: ["default"]
        highlight-style: "nord"
        incremental: true
        mermaid:
            theme: "default" 
        footer: "[Code and slides](https://www.github.com/abriz97/Computational_Training_ICL)"
        preview-links: true
        monofont: 'Source Code Pro'
        monofontoptions: 
            - Scale=0.8
        # code-block-bg: true
        code-line-numbers: false
        # code-block-border-left: "#31BAE9"
        # resources:
        #   - "name.qmd"
---

## Great R resources

In general:

1.  [Advanced R](https://adv-r.hadley.nz/)
2.  [R for Data Science](https://r4ds.hadley.nz/)
3.  [RStudio Cheatsheets](https://www.rstudio.com/resources/cheatsheets/)
4.  [Useful R packages](https://support.posit.co/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages)

Covering this session's topics

5.  [data.table](https://rdatatable.gitlab.io/data.table/)
6.  [parallel](https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/parallel.html#the-parallel-package)

## What is "fast code"?

1.  **Easy to write**\
    familiarity with code editor, libraries

2.  **Easy to understand**\
    structured, with consistent variable names, commented.

3.  **Easy to debug**\
    clear naming, DRY, tests.

4.  **Easy to run**\
    üèéÔ∏è (profiling, C++, using "optimized" code).

## Variable naming

::: r-stack
![](../figures/MagrittePipe.jpg){.fragment height="250" fig-align="center"}

![](../figures/catcat.png){.fragment height="250" fig-align="center"}
:::

. . .

names should be consistent, descriptive, lower case, readable.

. . .

For which snippet is it easier to guess the context?

::: columns
::: {.column width="50%"}
```{r}
tmp <-  10
tmp1 <- tmp * 24
```
:::

::: {.column width="50%"}
```{r}
cases_per_hour <- 10
cases_per_day <- cases_per_hour * 24
```
:::
:::

::: footer
[Dealing with the 2nd hardest thing in computer science (Patil)](https://indrajeetpatil.github.io/second-hardest-cs-thing/#/dealing-with-the-second-hardest-thing-in-computer-science)
:::

## Embrace functional programming I

*Functions are first-class citizens in R*

::: columns
::: {.column width="57%"}
-   can be passed as arguments
-   can be returned from other functions
-   can be assigned to variables
-   and more...
:::

::: {.column width="41%"}
```{r functional-programming-i }
#| filename: "first-class-citizenship.R"
#| warning: false

f <- function(x){x^2}

lapply(1:10, f)

generator <- function(n=2){
    function(x){x^n}
}
cube <- generator(3)

list(one_function = f)
```
:::
:::

## Embrace functional programming II

*Rethink for,while loops; "apply" instead*

. . .

> "To become significantly more reliable, code must become more transparent. In particular, nested conditions and loops must be viewed with great suspicion. Complicated control flows confuse programmers. Messy code often hides bugs."
>
> --- Bjarne Stroustrup

::: footer
[Advanced R, Funtional Programming (Wickham)](https://adv-r.hadley.nz/functionals.html)
:::

. . .

::: {style="text-align: right;"}
**...but why?**
:::

## 

Say you want to extract the $R^2$ from three linear models with different predictors (or formulae).

```{r }
#| filename: "formulae.R"
#| warning: false
formulae <- c(
    Sepal.Length ~ Sepal.Width,
    Sepal.Length ~ Petal.Length,
    Sepal.Length ~ Species
)
```

. . .

::: columns
::: {.column width="50%"}
```{r for-loop}
#| filename: "bad way"
#| warning: false
lm_results2 <- c()

for (formula in formulae) {
    fit <- lm(formula, data = iris)
    r2 <- summary(fit)$r.squared
    lm_results2 <- c(lm_results2, r2)
}
```
:::

::: {.column width="50%"}
```{r }
#| filename: "good way"
#| warning: false
extract_r2 <- function(formula) {
    fit <- lm(formula, data = iris)
    r2 <- summary(fit)$r.squared
    return(r2)
}

lm_results <- sapply(formulae, extract_r2)
```
:::
:::

::: {style="text-align: center;"}
What's the difference?
:::

. . .

```{r}
#| filename: "side effects"
#| warning: false
exists("fit")
```

::: footer
[Advanced R, Functional Programming (Wickham)](https://adv-r.hadley.nz/fp.html)
:::

::: notes
-   For loops may still be prefered (or the only choice), when the order of execution is important, and different runs may affect each other.
:::

## The `parallel` package

You can imagine wanting to run each of the apply/for loop iterations in `parallel`.

::: notes
especially if iterations are independent and computationally expensive. Two quick sentences on the two different approaches.
:::

::: columns
::: {.column width="50%"}
```{r socket}
#| filename: "sockets (not on Windows)"
#| warning: false

library(parallel)
f <- function(i) {
    lme4::lmer(
        Petal.Width ~ . - Species + (1 | Species),
        data = iris)
}

system.time(save1 <- lapply(1:100, f))
##    user  system elapsed
##   2.048   0.019   2.084
system.time(save2 <- mclapply(1:100, f))
##    user  system elapsed
##   1.295   0.150   1.471
```
:::

::: {.column width="50%"}
```{r }
#| filename: "forking"
#| warning: false

num_cores <- detectCores()
cl <- makeCluster(num_cores)
system.time(save3 <- parLapply(cl, 1:100, f))
#    user  system elapsed 
#   0.198   0.044   1.032 
stopCluster(cl)
```

-   requires further attention
:::
:::

::: footer
[Parallel computing in R](https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/parallel.html#the-parallel-package)
:::

## Introduction to [data.table](https://rdatatable.gitlab.io/data.table/)

-   `data.table` is a package that extends the data.frame class.
-   (Quicker) alternative to `dplyr` for large datasets.
-   [cheatsheet](https://rstudio.github.io/cheatsheets/datatable.pdf)

## all you need to know

### `dt[`[i]{style="color:blue;"}, [j]{style="color:red;"}, [by]{style="color:green;"}`]`

-   use the data.table called `dt` ...
-   subset it on the rows specified [i]{style="color:blue;"}...
-   and manipulate columns with [j]{style="color:red;"}...
-   grouped according to [by]{style="color:green;"}.

## Basic Examples with `iris`

#### Example 1: Subsetting and Summarizing

::: columns
::: {.column width="50%"}
-   explanation here.
:::

::: {.column width="50%"}
```{r}
library(data.table)
dt <- as.data.table(iris)
dt[Species == "setosa", mean(Sepal.Length)]
```
:::
:::

#### Example 2: Grouping and Aggregating

::: columns
::: {.column width="50%"}
-   explanation here.
:::

::: {.column width="50%"}
```{r}
library(data.table)
dt <- as.data.table(iris)
dt[Species == "setosa", mean(Sepal.Length)]
```
:::
:::

## Profiling

-   Identify (and hopefully fix!) bottlenecks in your code.
-   The [`profvis`](https://rstudio.github.io/profvis/) package is a good package to use for this purpose.

![](../figures/profiling.webp){.fragment height="350" fig-align="center"}

## Profiling Example: Column Means

```{r prof}
#| eval: true
#| output-location: slide
library(profvis)
library(data.table)
n <- 4e5
cols <- 150
data <- as.data.frame(x = matrix(rnorm(n * cols, mean = 5), ncol = cols))
data <- cbind(id = paste0("g", seq_len(n)), data)
dataDF <- as.data.table(data)
numeric_vars <- setdiff(names(data), "id")

profvis({
  means <- apply(data[, names(data) != "id"], 2, mean)
  means <- colMeans(data[, names(data) != "id"])
  means <- lapply(data[, names(data) != "id"], mean)
  means <- vapply(data[, names(data) != "id"], mean, numeric(1))
  means <- matrixStats::colMeans2(as.matrix(data[, names(data) != "id"]))
  means <- dataDF[, lapply(.SD, mean), .SDcols = numeric_vars]
})
```

## Good coding practice: Reproducibility & Generalisability

Why are code reproducibility & generalisability important?

-   Transparency & Verification
-   Collaboration & Longevity
-   Quicker detection of errors

## Reproducibility

-   Main idea: Be able to reproduce results to ensure they are valid.
-   Common practices:
    -   Setting the seed
    -   Importing all necessary packages
    -   Documenting R environment ([`sessionInfo()`](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/sessionInfo))

::: columns
::: {.column .fragment width="33%"}
```{r}
#| filename: "bad way"
#| error: false
x <- rnorm(1)
dtruncnorm(x, -5, 5, 0, 1)
```
:::

::: {.column .fragment width="33%"}
```{r}
#| filename: "still bad..."
#| error: false
set.seed(1234)
x <- rnorm(1)
dtruncnorm(x, -5, 5, 0, 1)
```
:::

::: {.column .fragment width="33%"}
```{r}
#| filename: "there we go!"
library(truncnorm)
set.seed(1234)
x <- rnorm(1)
dtruncnorm(x, -5, 5, 0, 1)
```
:::
:::

## Generalisability

-   Code should be "generalisable" meaning that anyone else can refer to it and use it on their own data.
-   Common practices:
    -   Write functions for operations you use frequently ‚öôÔ∏è
    -   Document your code (add comments) üí¨
    -   Add basic checks (testing) ‚ùì
    -   Test your code on different data sets ‚úîÔ∏è‚ùå
    
## Generalisability: Example

Task: Create a function that takes as input any data set and performs K-Means clustering (use function `kmeans`) of the observations using the reciprocals of variables (i.e. instead of using `x`, use `1/x`). Return the cluster assignment for the first 20 elements. This function will be used on the `iris` data set (with K=3 clusters). 

:::{.fragment}
```{r badex_gen}
#| filename: "don't even bother..."
#| eval: true
#| error: false
kmeans_recip <- function(){
  for (i in c(1:4)){
    iris[, i] <- 1/iris[, i]
  }
  kmeans_res <- kmeans(iris[, c(1:4)], centers = 3)
  return(kmeans_res$cluster)
}
kmeans_recip()[1:20]
```
:::

## Generalisability: Example
* The previous example works but it's really not generalisable...
* Obvious things that need improvement:
  + Hardcoded values (first 4 columns are numeric in `iris`)
  + Data set should be an input argument

:::{.fragment}
```{r okex_gen}
#| filename: "it's getting better"
#| eval: true
#| error: false
kmeans_recip <- function(data, cont_cols){
  for (i in cont_cols){
    data[, i] <- 1/data[, i]
  }
  kmeans_res <- kmeans(data[, cont_cols], centers = 3)
  return(kmeans_res$cluster)
}
kmeans_recip(data = iris, cont_cols = c(1:4))[1:20]
```
:::

## Generalisability: Example {.smaller}
* We can still do better! More things to consider:
  + Division by 0 is not allowed
  + Determine numerical variables automatically
  + Ensure there exists at least one continuous variable
  + Add some comments

:::{.fragment}
```{r goodex_gen}
#| filename: "that's looking good"
#| eval: true
#| error: false
kmeans_recip <- function(data){
  # Obtain numerical variables
  cont_cols <- which(sapply(data, is.numeric))
  # Check there is at least 1 numerical variable
  if (length(cont_cols)==0) stop('No numerical variables!')
  for (i in cont_cols){
    # Check if numerical variable takes 0 value
    ifelse(any(data[, i] == 0),
           stop('Division by 0 not allowed!'),
           data[, i] <- 1/data[, i])
  }
  # Apply K-Means clustering
  kmeans_res <- kmeans(data[, cont_cols], centers = 3)
  return(kmeans_res$cluster)
}
kmeans_recip(data = iris)[1:20]
```
:::

## Generalisability: Example
* Now we can also check if this works on the `diamonds` data set from the `ggplot2` package.

:::{.fragment}
```{r diamonds}
#| filename: "seems like it works!"
#| eval: true
#| error: true
library(ggplot2)
kmeans_recip(data = diamonds)[1:20]
```
:::

::: notes
{background-iframe="hello-matrix/index.html"} let us add this with the hex of data.table at the end.
:::
